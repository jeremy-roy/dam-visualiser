{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27b834e-b864-4744-ba5c-0be9d77c9877",
   "metadata": {},
   "source": [
    "# Western Cape Dam Levels\n",
    "Data has been downloaded from City of Cape Town Open Data Portal:  \n",
    "\n",
    "## GIS\n",
    "City of Cape Town Corporate GIS  \n",
    "https://odp-cctegis.opendata.arcgis.com/datasets/cctegis::bulk-water-dams-1/explore?location=-33.865920%2C19.071866%2C11.86  \n",
    "<br>\n",
    "Bulk Water dams (Bulk_Water_Dams.geojson):  \n",
    "https://odp-cctegis.opendata.arcgis.com/datasets/cctegis::bulk-water-dams-1/explore?location=-33.865920%2C19.071866%2C11.86  \n",
    "<br>\n",
    "## Timeseries\n",
    "Dam Levels (Dam_Levels_from_2012.csv):  \n",
    "https://odp-cctegis.opendata.arcgis.com/datasets/cctegis::dam-levels-from-2012/about  \n",
    "<br>\n",
    "(Not Useful)  \n",
    "Water Consumption (data/Water_consumption.xlsx):  \n",
    "https://odp-cctegis.opendata.arcgis.com/documents/cctegis::water-consumption-1/about  \n",
    "<br>\n",
    "(Not Useful)  \n",
    "Inland Water Quality Monthly Summary Report (Inland_WQ_Summary_Report.pdf):  \n",
    "https://odp-cctegis.opendata.arcgis.com/documents/cctegis::inland-water-quality-monthly-summary-report/about  \n",
    "<br>\n",
    "(Not Useful)  \n",
    "Rainfall Data From 2000 (Rainfall_Data_2000_to_2024.csv):  \n",
    "https://odp-cctegis.opendata.arcgis.com/datasets/cctegis::rainfall-data-from-2000-1/explore  \n",
    "\n",
    "## Weather Data\n",
    "meteostat python package \n",
    "\n",
    "## Poulations data\n",
    "https://www.macrotrends.net/global-metrics/cities/22481/cape-town/population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38fba9f-02a6-4d34-a3df-f0b1d7869532",
   "metadata": {},
   "source": [
    "### STEP 1: Dam Levels Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b062ec-61a2-4d69-ac6e-f35d39652719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load dam polygon GeoJSON\n",
    "gdf = gpd.read_file(\"data/2025-05-01/Bulk_Water_Dams.geojson\")\n",
    "\n",
    "# Load dam levels CSV, set date format\n",
    "df = pd.read_csv(\"data/2025-06-06/Dam_Levels_from_2012.csv\", encoding=\"ISO-8859-1\")\n",
    "df['DATE'] = df['DATE'].str.replace('Sept', 'Sep', regex=False)\n",
    "df['DATE'] = pd.to_datetime(df['DATE'], format='%d-%b-%y')\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.replace(r\"\\s+\", \"\", regex=True).str.lower()\n",
    "\n",
    "# Mapping from NAME in GeoJSON to CSV prefix (lowercase, no spaces)\n",
    "dam_name_mapping = {\n",
    "    \"Woodhead\": \"woodhead\",\n",
    "    \"Hely-Hutchinson\": \"hely-hutchinson\",\n",
    "    \"Lewis Gay\": \"lewisgay\",\n",
    "    \"Kleinplaats\": \"kleinplaats\",\n",
    "    \"Victoria\": \"victoria\",\n",
    "    \"Alexandra\": \"alexandra\",\n",
    "    \"De Villiers\": \"devilliers\",\n",
    "    \"Steenbras Lower\": \"steenbraslower\",\n",
    "    \"Steenbras Upper\": \"steenbrasupper\",\n",
    "    \"Voëlvlei\": \"voëlvlei\",\n",
    "    \"Wemmershoek\": \"wemmershoek\",\n",
    "    \"Theewaterskloof\": \"theewaterskloof\",\n",
    "    \"Berg River\": \"bergriver\",\n",
    "    \"Land-en-Zeezicht Dam\": \"land-enzeezicht\",\n",
    "    \"Big 5 Total\": \"totalstored-big5\",\n",
    "    \"Big 6 Total\": \"totalstored-big6\"\n",
    "}\n",
    "\n",
    "def build_timeseries(prefix):\n",
    "    # Find all columns related to this dam (that start with the prefix)\n",
    "    prefix_cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "\n",
    "    def find_col(keyword):\n",
    "        # Look for a column that contains the keyword (case-insensitive)\n",
    "        matches = [col for col in prefix_cols if keyword in col]\n",
    "        return matches[0] if matches else None\n",
    "\n",
    "    # Find matching columns\n",
    "    height_col = find_col(\"height\")\n",
    "    storage_col = find_col(\"storage\")\n",
    "    current_col = find_col(\"current\")\n",
    "    last_year_col = find_col(\"lastyear\")\n",
    "\n",
    "    # If we find no relevant columns, return empty\n",
    "    if not any([height_col, storage_col, current_col, last_year_col]):\n",
    "        return [], None\n",
    "\n",
    "    # Build DataFrame\n",
    "    cols = {'date': 'date'}\n",
    "    if height_col: cols[height_col] = 'height_m'\n",
    "    if storage_col: cols[storage_col] = 'storage_ml'\n",
    "    if current_col: cols[current_col] = 'percent_full'\n",
    "    if last_year_col: cols[last_year_col] = 'last_year_percent_full'\n",
    "\n",
    "    col_keys = list(cols.keys())\n",
    "    if 'date' not in col_keys:\n",
    "        col_keys = ['date'] + col_keys\n",
    "    ts = df[col_keys].copy()\n",
    "    # ts['date'] = pd.to_datetime(ts['date']).dt.strftime('%Y-%m-%d')  # ensure datetime\n",
    "    ts.rename(columns=cols, inplace=True)\n",
    "\n",
    "    # Ensure numeric columns are truly numeric\n",
    "    for col in ['height_m', 'storage_ml', 'percent_full', 'last_year_percent_full']:\n",
    "        if col in ts.columns:\n",
    "            ts[col] = pd.to_numeric(ts[col], errors='coerce')\n",
    "\n",
    "    # format nulls\n",
    "    ts = ts.where(pd.notnull(ts), None)\n",
    "\n",
    "    return ts\n",
    "\n",
    "\n",
    "# Create output containers\n",
    "dam_ts_daily = {}\n",
    "dam_ts_monthly = {}\n",
    "dam_ts_yearly = {}\n",
    "\n",
    "for dam_name, prefix in dam_name_mapping.items():\n",
    "    df_ts = build_timeseries(prefix)\n",
    "    if df_ts is None or df_ts.empty:\n",
    "        print('no data set found')\n",
    "        continue\n",
    "\n",
    "    df_ts['date'] = pd.to_datetime(df_ts['date'])\n",
    "\n",
    "    # DAILY\n",
    "    df_ts_sorted = df_ts.sort_values('date')\n",
    "    df_ts_sorted = df_ts_sorted.round(2)\n",
    "    df_ts_sorted['date'] = df_ts_sorted['date'].dt.strftime('%Y-%m-%d')\n",
    "    dam_ts_daily[prefix] = df_ts_sorted.where(pd.notnull(df_ts_sorted), None).to_dict(orient='records')\n",
    "\n",
    "    # MONTHLY\n",
    "    monthly = df_ts.resample('ME', on='date').mean(numeric_only=True).reset_index()\n",
    "    monthly = monthly.round(2)\n",
    "    monthly['date'] = monthly['date'].dt.strftime('%Y-%m')\n",
    "    dam_ts_monthly[prefix] = monthly.where(pd.notnull(monthly), None).to_dict(orient='records')\n",
    "\n",
    "    # YEARLY\n",
    "    yearly = df_ts.resample('YE', on='date').mean(numeric_only=True).reset_index()\n",
    "    yearly = yearly.round(2)\n",
    "    yearly['date'] = yearly['date'].dt.strftime('%Y')\n",
    "    dam_ts_yearly[prefix] = yearly.where(pd.notnull(yearly), None).to_dict(orient='records')\n",
    "\n",
    "\n",
    "# Update GeoJSON properties with centroid\n",
    "for i, row in gdf.iterrows():\n",
    "    if row[\"geometry\"]:\n",
    "        centroid = row[\"geometry\"].centroid\n",
    "        gdf.at[i, 'centroid'] = Point(centroid.x, centroid.y)\n",
    "    else:\n",
    "        gdf.at[i, 'centroid'] = None\n",
    "\n",
    "# Clean NaNs recursively\n",
    "def clean_nans(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_nans(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_nans(v) for v in obj]\n",
    "    elif isinstance(obj, float) and (np.isnan(v := obj)):\n",
    "        return None\n",
    "    return obj\n",
    "\n",
    "# Save Timeseries data\n",
    "os.makedirs(\"output/timeseries\", exist_ok=True)\n",
    "\n",
    "with open(\"output/timeseries/dam_levels_daily.json\", \"w\") as f:\n",
    "    json.dump(clean_nans(dam_ts_daily), f, indent=2)\n",
    "\n",
    "with open(\"output/timeseries/dam_levels_monthly.json\", \"w\") as f:\n",
    "    json.dump(clean_nans(dam_ts_monthly), f, indent=2)\n",
    "\n",
    "with open(\"output/timeseries/dam_levels_yearly.json\", \"w\") as f:\n",
    "    json.dump(clean_nans(dam_ts_yearly), f, indent=2)\n",
    "\n",
    "# Save enriched GeoJSON\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "gdf.to_file(\"output/Bulk_Water_Dams_Enriched.geojson\", driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282339e-12cd-469f-88ee-e4322c1f85da",
   "metadata": {},
   "source": [
    "### STEP 2: Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419c3005-73ba-4505-b022-13809028c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "def clean_nans(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_nans(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_nans(v) for v in obj]\n",
    "    elif isinstance(obj, float) and (np.isnan(v := obj)):\n",
    "        return None\n",
    "    return obj\n",
    "\n",
    "\n",
    "# Define Cape Town coordinates\n",
    "cape_town = Point(-33.9258, 18.4232)\n",
    "\n",
    "# Time range: last 20 years\n",
    "start = datetime(2000, 1, 1)\n",
    "end = datetime.today()\n",
    "\n",
    "# Fetch daily weather data\n",
    "data = Daily(cape_town, start, end)\n",
    "data = data.fetch()\n",
    "\n",
    "# Filter for average temperature and precipitation\n",
    "df = data[['tavg', 'prcp']].copy()\n",
    "\n",
    "# resample monthly\n",
    "monthly = df.resample('ME').mean()\n",
    "monthly['prcp'] = df['prcp'].resample('ME').sum()\n",
    "\n",
    "# resample yearly\n",
    "yearly = df.resample('YE').mean()\n",
    "yearly['prcp'] = df['prcp'].resample('YE').sum()\n",
    "\n",
    "# Add date columns for export\n",
    "df = df.copy()\n",
    "df = df.sort_index()\n",
    "df['date'] = df.index.strftime('%Y-%m-%d')\n",
    "daily_out = df.reset_index(drop=True)[['date', 'tavg', 'prcp']]\n",
    "\n",
    "monthly = monthly.copy()\n",
    "monthly['date'] = monthly.index.strftime('%Y-%m')\n",
    "monthly_out = monthly.reset_index(drop=True)[['date', 'tavg', 'prcp']]\n",
    "\n",
    "yearly = yearly.copy()\n",
    "yearly['date'] = yearly.index.strftime('%Y')\n",
    "yearly_out = yearly.reset_index(drop=True)[['date', 'tavg', 'prcp']]\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"output/timeseries\", exist_ok=True)\n",
    "\n",
    "with open(\"output/timeseries/cape_town_rainfall_daily.json\", \"w\") as f:\n",
    "    json.dump(clean_nans(daily_out.to_dict(orient='records')), f, indent=2)\n",
    "\n",
    "with open(\"output/timeseries/cape_town_rainfall_monthly.json\", \"w\") as f:\n",
    "    json.dump(clean_nans(monthly_out.to_dict(orient='records')), f, indent=2)\n",
    "\n",
    "with open(\"output/timeseries/cape_town_rainfall_yearly.json\", \"w\") as f:\n",
    "    json.dump(clean_nans(yearly_out.to_dict(orient='records')), f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bdd511-7dbe-4b5f-b9bb-9b09a90855cd",
   "metadata": {},
   "source": [
    "### STEP 3: Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f8f1323-db5e-42b2-8fd8-5460f0ba26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "annualPpopulationData = 'data/2025-05-19/Cape-Town-Population-Total-Population-By-Year-2025-05-17-22-32.csv'\n",
    "\n",
    "# Load dam levels CSV\n",
    "df = pd.read_csv(annualPpopulationData)\n",
    "df.rename(columns={'Unnamed: 0': 'year'}, inplace=True)\n",
    "# df['year'] = pd.to_datetime(df['year'])\n",
    "df.columns = ['year', 'population']\n",
    "\n",
    "df = df[['year', 'population']].dropna().sort_values('year')\n",
    "\n",
    "os.makedirs(\"output/timeseries\", exist_ok=True)\n",
    "\n",
    "with open(\"output/timeseries/cape_town_population_yearly.json\", \"w\") as f:\n",
    "    json.dump(df.to_dict(orient='records'), f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
